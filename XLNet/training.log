2022-02-22 12:01:48,634 ----------------------------------------------------------------------------------------------------
2022-02-22 12:01:48,634 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): XLNetEmbeddings(
      model=0-xlnet-large-cased
      (model): XLNetModel(
        (word_embedding): Embedding(32000, 1024)
        (layer): ModuleList(
          (0): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (3): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (4): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (5): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (6): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (7): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (8): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (9): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (10): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (11): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (12): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (13): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (14): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (15): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (16): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (17): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (18): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (19): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (20): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (21): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (22): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (23): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=2048, out_features=2048, bias=True)
  (rnn): LSTM(2048, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (linear): Linear(in_features=256, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2022-02-22 12:01:48,656 ----------------------------------------------------------------------------------------------------
2022-02-22 12:01:48,656 Corpus: "Corpus: 1000 train + 500 dev + 500 test sentences"
2022-02-22 12:01:48,656 ----------------------------------------------------------------------------------------------------
2022-02-22 12:01:48,656 Parameters:
2022-02-22 12:01:48,656  - learning_rate: "0.05"
2022-02-22 12:01:48,656  - mini_batch_size: "4"
2022-02-22 12:01:48,656  - patience: "4"
2022-02-22 12:01:48,656  - anneal_factor: "0.5"
2022-02-22 12:01:48,656  - max_epochs: "100"
2022-02-22 12:01:48,656  - shuffle: "True"
2022-02-22 12:01:48,656  - train_with_dev: "False"
2022-02-22 12:01:48,656  - batch_growth_annealing: "False"
2022-02-22 12:01:48,656 ----------------------------------------------------------------------------------------------------
2022-02-22 12:01:48,656 Model training base path: "C:\Users\W10\PycharmProjects\terminology-extraction\XLNet___downsample_train_0.0_bs_4_lr_0.05_af_0.5_p_4_hsize_128_crf_0_lrnn_2_dp_0.0_wdp_0.05_ldp_0.5"
2022-02-22 12:01:48,656 ----------------------------------------------------------------------------------------------------
2022-02-22 12:01:48,656 Device: cpu
2022-02-22 12:01:48,656 ----------------------------------------------------------------------------------------------------
2022-02-22 12:01:48,656 Embeddings storage mode: cpu
2022-02-22 12:01:48,656 ----------------------------------------------------------------------------------------------------
2022-02-22 12:05:28,540 epoch 1 - iter 25/250 - loss 0.67663907 - samples/sec: 0.45 - lr: 0.050000
2022-02-22 12:09:19,950 epoch 1 - iter 50/250 - loss 0.55424861 - samples/sec: 0.43 - lr: 0.050000
2022-02-22 12:12:36,258 epoch 1 - iter 75/250 - loss 0.49968736 - samples/sec: 0.51 - lr: 0.050000
2022-02-22 12:16:39,779 epoch 1 - iter 100/250 - loss 0.46834715 - samples/sec: 0.41 - lr: 0.050000
2022-02-22 12:20:37,339 epoch 1 - iter 125/250 - loss 0.44377366 - samples/sec: 0.42 - lr: 0.050000
2022-02-22 12:24:08,522 epoch 1 - iter 150/250 - loss 0.42782590 - samples/sec: 0.47 - lr: 0.050000
2022-02-22 12:27:53,695 epoch 1 - iter 175/250 - loss 0.41561190 - samples/sec: 0.44 - lr: 0.050000
2022-02-22 12:31:47,139 epoch 1 - iter 200/250 - loss 0.40193440 - samples/sec: 0.43 - lr: 0.050000
2022-02-22 12:35:43,066 epoch 1 - iter 225/250 - loss 0.39275431 - samples/sec: 0.42 - lr: 0.050000
2022-02-22 12:39:29,855 epoch 1 - iter 250/250 - loss 0.38583744 - samples/sec: 0.44 - lr: 0.050000
2022-02-22 12:39:29,863 ----------------------------------------------------------------------------------------------------
2022-02-22 12:39:29,863 EPOCH 1 done: loss 0.3858 - lr 0.0500000
2022-02-22 12:55:49,978 DEV : loss 0.3236653804779053 - f1-score (micro avg)  0.4292
2022-02-22 12:55:50,077 BAD EPOCHS (no improvement): 0
2022-02-22 12:55:50,079 saving best model
2022-02-22 12:55:56,204 ----------------------------------------------------------------------------------------------------
2022-02-22 12:56:54,643 epoch 2 - iter 25/250 - loss 0.31387982 - samples/sec: 1.71 - lr: 0.050000
2022-02-22 12:57:45,131 epoch 2 - iter 50/250 - loss 0.31027796 - samples/sec: 1.98 - lr: 0.050000
2022-02-22 12:58:27,002 epoch 2 - iter 75/250 - loss 0.30735042 - samples/sec: 2.39 - lr: 0.050000
2022-02-22 12:59:20,791 epoch 2 - iter 100/250 - loss 0.30362180 - samples/sec: 1.86 - lr: 0.050000
2022-02-22 13:00:16,667 epoch 2 - iter 125/250 - loss 0.30656327 - samples/sec: 1.79 - lr: 0.050000
2022-02-22 13:01:06,255 epoch 2 - iter 150/250 - loss 0.30555651 - samples/sec: 2.02 - lr: 0.050000
2022-02-22 13:01:56,001 epoch 2 - iter 175/250 - loss 0.30221333 - samples/sec: 2.01 - lr: 0.050000
2022-02-22 13:02:47,732 epoch 2 - iter 200/250 - loss 0.29967338 - samples/sec: 1.93 - lr: 0.050000
2022-02-22 13:03:38,721 epoch 2 - iter 225/250 - loss 0.29907249 - samples/sec: 1.96 - lr: 0.050000
2022-02-22 13:04:30,104 epoch 2 - iter 250/250 - loss 0.29864947 - samples/sec: 1.95 - lr: 0.050000
2022-02-22 13:04:30,112 ----------------------------------------------------------------------------------------------------
2022-02-22 13:04:30,112 EPOCH 2 done: loss 0.2986 - lr 0.0500000
2022-02-22 13:04:49,422 DEV : loss 0.26330074667930603 - f1-score (micro avg)  0.3201
2022-02-22 13:04:49,532 BAD EPOCHS (no improvement): 1
2022-02-22 13:04:49,532 ----------------------------------------------------------------------------------------------------
2022-02-22 13:05:40,767 epoch 3 - iter 25/250 - loss 0.28202866 - samples/sec: 1.95 - lr: 0.050000
2022-02-22 13:06:29,418 epoch 3 - iter 50/250 - loss 0.27902347 - samples/sec: 2.06 - lr: 0.050000
2022-02-22 13:07:19,677 epoch 3 - iter 75/250 - loss 0.28075312 - samples/sec: 1.99 - lr: 0.050000
2022-02-22 13:08:11,957 epoch 3 - iter 100/250 - loss 0.27885893 - samples/sec: 1.91 - lr: 0.050000
2022-02-22 13:09:01,196 epoch 3 - iter 125/250 - loss 0.28152833 - samples/sec: 2.03 - lr: 0.050000
2022-02-22 13:09:51,146 epoch 3 - iter 150/250 - loss 0.28141994 - samples/sec: 2.00 - lr: 0.050000
2022-02-22 13:10:42,378 epoch 3 - iter 175/250 - loss 0.27829979 - samples/sec: 1.95 - lr: 0.050000
2022-02-22 13:11:36,221 epoch 3 - iter 200/250 - loss 0.27830878 - samples/sec: 1.86 - lr: 0.050000
2022-02-22 13:12:24,415 epoch 3 - iter 225/250 - loss 0.27802824 - samples/sec: 2.07 - lr: 0.050000
2022-02-22 13:13:19,929 epoch 3 - iter 250/250 - loss 0.27817788 - samples/sec: 1.80 - lr: 0.050000
2022-02-22 13:13:19,929 ----------------------------------------------------------------------------------------------------
2022-02-22 13:13:19,929 EPOCH 3 done: loss 0.2782 - lr 0.0500000
2022-02-22 13:13:39,766 DEV : loss 0.25507408380508423 - f1-score (micro avg)  0.3581
2022-02-22 13:13:39,865 BAD EPOCHS (no improvement): 2
2022-02-22 13:13:39,867 ----------------------------------------------------------------------------------------------------
2022-02-22 13:14:34,562 epoch 4 - iter 25/250 - loss 0.28096719 - samples/sec: 1.83 - lr: 0.050000
2022-02-22 13:15:25,617 epoch 4 - iter 50/250 - loss 0.28130639 - samples/sec: 1.96 - lr: 0.050000
2022-02-22 13:16:20,558 epoch 4 - iter 75/250 - loss 0.26953422 - samples/sec: 1.82 - lr: 0.050000
2022-02-22 13:17:09,530 epoch 4 - iter 100/250 - loss 0.27100444 - samples/sec: 2.04 - lr: 0.050000
2022-02-22 13:17:53,941 epoch 4 - iter 125/250 - loss 0.26848406 - samples/sec: 2.25 - lr: 0.050000
2022-02-22 13:18:38,350 epoch 4 - iter 150/250 - loss 0.27015151 - samples/sec: 2.25 - lr: 0.050000
2022-02-22 13:19:28,265 epoch 4 - iter 175/250 - loss 0.26975887 - samples/sec: 2.00 - lr: 0.050000
2022-02-22 13:20:24,099 epoch 4 - iter 200/250 - loss 0.26785596 - samples/sec: 1.79 - lr: 0.050000
2022-02-22 13:21:15,064 epoch 4 - iter 225/250 - loss 0.26803372 - samples/sec: 1.96 - lr: 0.050000
2022-02-22 13:22:06,425 epoch 4 - iter 250/250 - loss 0.26641824 - samples/sec: 1.95 - lr: 0.050000
2022-02-22 13:22:06,426 ----------------------------------------------------------------------------------------------------
2022-02-22 13:22:06,427 EPOCH 4 done: loss 0.2664 - lr 0.0500000
2022-02-22 13:22:26,298 DEV : loss 0.2586861252784729 - f1-score (micro avg)  0.4292
2022-02-22 13:22:26,418 BAD EPOCHS (no improvement): 0
2022-02-22 13:22:26,418 saving best model
2022-02-22 13:22:34,436 ----------------------------------------------------------------------------------------------------
2022-02-22 13:23:26,335 epoch 5 - iter 25/250 - loss 0.25606676 - samples/sec: 1.93 - lr: 0.050000
2022-02-22 13:24:16,538 epoch 5 - iter 50/250 - loss 0.26333120 - samples/sec: 1.99 - lr: 0.050000
2022-02-22 13:25:03,711 epoch 5 - iter 75/250 - loss 0.26204213 - samples/sec: 2.12 - lr: 0.050000
2022-02-22 13:25:52,446 epoch 5 - iter 100/250 - loss 0.26309769 - samples/sec: 2.05 - lr: 0.050000
2022-02-22 13:26:43,543 epoch 5 - iter 125/250 - loss 0.26375982 - samples/sec: 1.96 - lr: 0.050000
2022-02-22 13:27:35,025 epoch 5 - iter 150/250 - loss 0.26030386 - samples/sec: 1.94 - lr: 0.050000
2022-02-22 13:28:25,625 epoch 5 - iter 175/250 - loss 0.26018052 - samples/sec: 1.98 - lr: 0.050000
2022-02-22 13:29:15,725 epoch 5 - iter 200/250 - loss 0.25762617 - samples/sec: 2.00 - lr: 0.050000
2022-02-22 13:30:04,468 epoch 5 - iter 225/250 - loss 0.25601289 - samples/sec: 2.05 - lr: 0.050000
2022-02-22 13:30:52,481 epoch 5 - iter 250/250 - loss 0.25776255 - samples/sec: 2.08 - lr: 0.050000
2022-02-22 13:30:52,481 ----------------------------------------------------------------------------------------------------
2022-02-22 13:30:52,481 EPOCH 5 done: loss 0.2578 - lr 0.0500000
2022-02-22 13:31:12,778 DEV : loss 0.24848805367946625 - f1-score (micro avg)  0.3897
2022-02-22 13:31:12,894 BAD EPOCHS (no improvement): 1
2022-02-22 13:31:12,894 ----------------------------------------------------------------------------------------------------
2022-02-22 13:32:03,126 epoch 6 - iter 25/250 - loss 0.24055589 - samples/sec: 1.99 - lr: 0.050000
2022-02-22 13:32:51,702 epoch 6 - iter 50/250 - loss 0.24667738 - samples/sec: 2.06 - lr: 0.050000
2022-02-22 13:33:43,473 epoch 6 - iter 75/250 - loss 0.25140003 - samples/sec: 1.93 - lr: 0.050000
2022-02-22 13:34:32,192 epoch 6 - iter 100/250 - loss 0.25262576 - samples/sec: 2.05 - lr: 0.050000
2022-02-22 13:35:25,118 epoch 6 - iter 125/250 - loss 0.25303646 - samples/sec: 1.89 - lr: 0.050000
2022-02-22 13:36:14,028 epoch 6 - iter 150/250 - loss 0.25255421 - samples/sec: 2.04 - lr: 0.050000
2022-02-22 13:37:02,172 epoch 6 - iter 175/250 - loss 0.25110087 - samples/sec: 2.08 - lr: 0.050000
2022-02-22 13:37:52,991 epoch 6 - iter 200/250 - loss 0.25120071 - samples/sec: 1.97 - lr: 0.050000
2022-02-22 13:38:38,672 epoch 6 - iter 225/250 - loss 0.25181858 - samples/sec: 2.19 - lr: 0.050000
2022-02-22 13:39:31,539 epoch 6 - iter 250/250 - loss 0.25198895 - samples/sec: 1.89 - lr: 0.050000
2022-02-22 13:39:31,547 ----------------------------------------------------------------------------------------------------
2022-02-22 13:39:31,547 EPOCH 6 done: loss 0.2520 - lr 0.0500000
2022-02-22 13:39:51,362 DEV : loss 0.2486599087715149 - f1-score (micro avg)  0.4087
2022-02-22 13:39:51,465 BAD EPOCHS (no improvement): 2
2022-02-22 13:39:51,465 ----------------------------------------------------------------------------------------------------
2022-02-22 13:40:39,234 epoch 7 - iter 25/250 - loss 0.23677001 - samples/sec: 2.09 - lr: 0.050000
2022-02-22 13:41:32,713 epoch 7 - iter 50/250 - loss 0.24246694 - samples/sec: 1.87 - lr: 0.050000
2022-02-22 13:42:18,433 epoch 7 - iter 75/250 - loss 0.24640329 - samples/sec: 2.19 - lr: 0.050000
2022-02-22 13:43:10,620 epoch 7 - iter 100/250 - loss 0.24283513 - samples/sec: 1.92 - lr: 0.050000
2022-02-22 13:44:00,052 epoch 7 - iter 125/250 - loss 0.24105762 - samples/sec: 2.02 - lr: 0.050000
2022-02-22 13:44:50,747 epoch 7 - iter 150/250 - loss 0.24142705 - samples/sec: 1.97 - lr: 0.050000
2022-02-22 13:45:40,505 epoch 7 - iter 175/250 - loss 0.24043946 - samples/sec: 2.01 - lr: 0.050000
2022-02-22 13:46:30,144 epoch 7 - iter 200/250 - loss 0.24273868 - samples/sec: 2.01 - lr: 0.050000
2022-02-22 13:47:22,342 epoch 7 - iter 225/250 - loss 0.24196171 - samples/sec: 1.92 - lr: 0.050000
2022-02-22 13:48:13,022 epoch 7 - iter 250/250 - loss 0.24377090 - samples/sec: 1.97 - lr: 0.050000
2022-02-22 13:48:13,022 ----------------------------------------------------------------------------------------------------
2022-02-22 13:48:13,022 EPOCH 7 done: loss 0.2438 - lr 0.0500000
2022-02-22 13:48:32,820 DEV : loss 0.2558049261569977 - f1-score (micro avg)  0.4571
2022-02-22 13:48:32,939 BAD EPOCHS (no improvement): 0
2022-02-22 13:48:32,941 saving best model
2022-02-22 13:48:38,751 ----------------------------------------------------------------------------------------------------
2022-02-22 13:49:26,514 epoch 8 - iter 25/250 - loss 0.23053775 - samples/sec: 2.09 - lr: 0.050000
2022-02-22 13:50:22,180 epoch 8 - iter 50/250 - loss 0.23722247 - samples/sec: 1.80 - lr: 0.050000
2022-02-22 14:14:33,630 epoch 8 - iter 75/250 - loss 0.24036912 - samples/sec: 0.07 - lr: 0.050000
2022-02-22 14:15:20,923 epoch 8 - iter 100/250 - loss 0.24293474 - samples/sec: 2.11 - lr: 0.050000
2022-02-22 14:16:15,192 epoch 8 - iter 125/250 - loss 0.24000858 - samples/sec: 1.84 - lr: 0.050000
2022-02-22 14:17:05,744 epoch 8 - iter 150/250 - loss 0.24095318 - samples/sec: 1.98 - lr: 0.050000
2022-02-22 14:17:59,893 epoch 8 - iter 175/250 - loss 0.24208591 - samples/sec: 1.85 - lr: 0.050000
2022-02-22 14:18:50,346 epoch 8 - iter 200/250 - loss 0.24065384 - samples/sec: 1.98 - lr: 0.050000
2022-02-22 14:19:43,766 epoch 8 - iter 225/250 - loss 0.24149109 - samples/sec: 1.87 - lr: 0.050000
2022-02-22 14:20:34,153 epoch 8 - iter 250/250 - loss 0.24083622 - samples/sec: 1.99 - lr: 0.050000
2022-02-22 14:20:34,153 ----------------------------------------------------------------------------------------------------
2022-02-22 14:20:34,153 EPOCH 8 done: loss 0.2408 - lr 0.0500000
2022-02-22 14:20:49,254 ----------------------------------------------------------------------------------------------------
2022-02-22 14:20:49,254 Exiting from training early.
2022-02-22 14:20:49,254 Saving model ...
2022-02-22 14:20:52,126 Done.
2022-02-22 14:20:52,126 ----------------------------------------------------------------------------------------------------
2022-02-22 14:20:52,142 loading file C:\Users\W10\PycharmProjects\terminology-extraction\XLNet___downsample_train_0.0_bs_4_lr_0.05_af_0.5_p_4_hsize_128_crf_0_lrnn_2_dp_0.0_wdp_0.05_ldp_0.5\best-model.pt
2022-02-22 14:36:29,744 0.4325	0.5434	0.4816	0.3172
2022-02-22 14:36:29,744 
Results:
- F-score (micro) 0.4816
- F-score (macro) 0.4816
- Accuracy 0.3172

By class:
              precision    recall  f1-score   support

         KEY     0.4325    0.5434    0.4816      4844

   micro avg     0.4325    0.5434    0.4816      4844
   macro avg     0.4325    0.5434    0.4816      4844
weighted avg     0.4325    0.5434    0.4816      4844
 samples avg     0.3172    0.3172    0.3172      4844

2022-02-22 14:36:29,744 ----------------------------------------------------------------------------------------------------
